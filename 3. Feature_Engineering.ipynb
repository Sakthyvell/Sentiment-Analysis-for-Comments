{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df = pandas.read_csv(\"/home/sakthy1497/Downloads/Amazon_Review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>complaint</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>positive</td>\n",
       "      <td>feedback</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>positive</td>\n",
       "      <td>feedback</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>negative</td>\n",
       "      <td>complaint</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>positive</td>\n",
       "      <td>feedback</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments Sentiment     Intent  \\\n",
       "0  So there is no way for me to plug it in here i...  negative  complaint   \n",
       "1                        Good case, Excellent value.  positive   feedback   \n",
       "2                             Great for the jawbone.  positive   feedback   \n",
       "3  Tied to charger for conversations lasting more...  negative  complaint   \n",
       "4                                  The mic is great.  positive   feedback   \n",
       "\n",
       "   Context  \n",
       "0   others  \n",
       "1  quality  \n",
       "2  quality  \n",
       "3  quality  \n",
       "4  quality  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       So there is no way for me to plug it in here i...\n",
       "1                             Good case, Excellent value.\n",
       "2                                  Great for the jawbone.\n",
       "3       Tied to charger for conversations lasting more...\n",
       "4                                       The mic is great.\n",
       "                              ...                        \n",
       "1995    I think food should have flavor and texture an...\n",
       "1996                             Appetite instantly gone.\n",
       "1997    Overall I was not impressed and would not go b...\n",
       "1998    The whole experience was underwhelming, and I ...\n",
       "1999    Then, as if I hadn't wasted enough of my life ...\n",
       "Name: Comments, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Special character cleaning\n",
    "\n",
    "We can see the following special characters:\n",
    "\n",
    "* ``\\r``\n",
    "* ``\\n``\n",
    "* ``\\`` before possessive pronouns (`government's = government\\'s`)\n",
    "* ``\\`` before possessive pronouns 2 (`Yukos'` = `Yukos\\'`)\n",
    "* ``\"`` when quoting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Content_Parsed_1'] = df['Comments'].str.replace(\"\\r\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Upcase/downcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Punctuation signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;\")\n",
    "df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Possessive pronouns\n",
    "We'll also remove possessive pronoun terminations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Stemming and Lemmatization\n",
    "\n",
    "Since stemming can produce output words that don't exist, we'll only use a lemmatization process at this moment. Lemmatization takes into consideration the morphological analysis of the words and returns words that do exist, so it will be more useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sakthy1497/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sakthy1497/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['Content_Parsed_4']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 8 columns):\n",
      "Comments            2000 non-null object\n",
      "Sentiment           2000 non-null object\n",
      "Intent              2000 non-null object\n",
      "Context             2000 non-null object\n",
      "Content_Parsed_1    2000 non-null object\n",
      "Content_Parsed_2    2000 non-null object\n",
      "Content_Parsed_3    2000 non-null object\n",
      "Content_Parsed_4    2000 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 125.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmatized_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_5'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sakthy1497/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))\n",
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Context</th>\n",
       "      <th>Content_Parsed_1</th>\n",
       "      <th>Content_Parsed_2</th>\n",
       "      <th>Content_Parsed_3</th>\n",
       "      <th>Content_Parsed_4</th>\n",
       "      <th>Content_Parsed_5</th>\n",
       "      <th>Content_Parsed_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>complaint</td>\n",
       "      <td>others</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>so there is no way for me to plug it in here i...</td>\n",
       "      <td>so there is no way for me to plug it in here i...</td>\n",
       "      <td>so there is no way for me to plug it in here i...</td>\n",
       "      <td>so there be no way for me to plug it in here i...</td>\n",
       "      <td>way    plug      us unless  go   converter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>positive</td>\n",
       "      <td>feedback</td>\n",
       "      <td>quality</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>good case, excellent value.</td>\n",
       "      <td>good case excellent value</td>\n",
       "      <td>good case excellent value</td>\n",
       "      <td>good case excellent value</td>\n",
       "      <td>good case excellent value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>positive</td>\n",
       "      <td>feedback</td>\n",
       "      <td>quality</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>great for the jawbone.</td>\n",
       "      <td>great for the jawbone</td>\n",
       "      <td>great for the jawbone</td>\n",
       "      <td>great for the jawbone</td>\n",
       "      <td>great   jawbone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>negative</td>\n",
       "      <td>complaint</td>\n",
       "      <td>quality</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>tied to charger for conversations lasting more...</td>\n",
       "      <td>tied to charger for conversations lasting more...</td>\n",
       "      <td>tied to charger for conversations lasting more...</td>\n",
       "      <td>tie to charger for conversations last more tha...</td>\n",
       "      <td>tie  charger  conversations last   45 minutesm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>positive</td>\n",
       "      <td>feedback</td>\n",
       "      <td>quality</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>the mic is great.</td>\n",
       "      <td>the mic is great</td>\n",
       "      <td>the mic is great</td>\n",
       "      <td>the mic be great</td>\n",
       "      <td>mic  great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments Sentiment     Intent  \\\n",
       "0  So there is no way for me to plug it in here i...  negative  complaint   \n",
       "1                        Good case, Excellent value.  positive   feedback   \n",
       "2                             Great for the jawbone.  positive   feedback   \n",
       "3  Tied to charger for conversations lasting more...  negative  complaint   \n",
       "4                                  The mic is great.  positive   feedback   \n",
       "\n",
       "   Context                                   Content_Parsed_1  \\\n",
       "0   others  So there is no way for me to plug it in here i...   \n",
       "1  quality                        Good case, Excellent value.   \n",
       "2  quality                             Great for the jawbone.   \n",
       "3  quality  Tied to charger for conversations lasting more...   \n",
       "4  quality                                  The mic is great.   \n",
       "\n",
       "                                    Content_Parsed_2  \\\n",
       "0  so there is no way for me to plug it in here i...   \n",
       "1                        good case, excellent value.   \n",
       "2                             great for the jawbone.   \n",
       "3  tied to charger for conversations lasting more...   \n",
       "4                                  the mic is great.   \n",
       "\n",
       "                                    Content_Parsed_3  \\\n",
       "0  so there is no way for me to plug it in here i...   \n",
       "1                          good case excellent value   \n",
       "2                              great for the jawbone   \n",
       "3  tied to charger for conversations lasting more...   \n",
       "4                                   the mic is great   \n",
       "\n",
       "                                    Content_Parsed_4  \\\n",
       "0  so there is no way for me to plug it in here i...   \n",
       "1                          good case excellent value   \n",
       "2                              great for the jawbone   \n",
       "3  tied to charger for conversations lasting more...   \n",
       "4                                   the mic is great   \n",
       "\n",
       "                                    Content_Parsed_5  \\\n",
       "0  so there be no way for me to plug it in here i...   \n",
       "1                          good case excellent value   \n",
       "2                              great for the jawbone   \n",
       "3  tie to charger for conversations last more tha...   \n",
       "4                                   the mic be great   \n",
       "\n",
       "                                    Content_Parsed_6  \n",
       "0         way    plug      us unless  go   converter  \n",
       "1                          good case excellent value  \n",
       "2                                    great   jawbone  \n",
       "3  tie  charger  conversations last   45 minutesm...  \n",
       "4                                         mic  great  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Context</th>\n",
       "      <th>Content_Parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>complaint</td>\n",
       "      <td>negative</td>\n",
       "      <td>others</td>\n",
       "      <td>way    plug      us unless  go   converter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>feedback</td>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>good case excellent value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>feedback</td>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>great   jawbone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>complaint</td>\n",
       "      <td>negative</td>\n",
       "      <td>quality</td>\n",
       "      <td>tie  charger  conversations last   45 minutesm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>feedback</td>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>mic  great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments     Intent Sentiment  \\\n",
       "0  So there is no way for me to plug it in here i...  complaint  negative   \n",
       "1                        Good case, Excellent value.   feedback  positive   \n",
       "2                             Great for the jawbone.   feedback  positive   \n",
       "3  Tied to charger for conversations lasting more...  complaint  negative   \n",
       "4                                  The mic is great.   feedback  positive   \n",
       "\n",
       "   Context                                     Content_Parsed  \n",
       "0   others         way    plug      us unless  go   converter  \n",
       "1  quality                          good case excellent value  \n",
       "2  quality                                    great   jawbone  \n",
       "3  quality  tie  charger  conversations last   45 minutesm...  \n",
       "4  quality                                         mic  great  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_columns = [\"Comments\", \"Intent\", \"Sentiment\", \"Context\", \"Content_Parsed_6\"]\n",
    "df = df[list_columns]\n",
    "\n",
    "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label coding\n",
    "We'll create a dictionary with the label codification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_codes = {\n",
    "    'feedback' : 0,\n",
    "    'complaint' : 1,\n",
    "    'spam' : 2,\n",
    "    'marketing' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Context</th>\n",
       "      <th>Content_Parsed</th>\n",
       "      <th>Intent_Codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>complaint</td>\n",
       "      <td>negative</td>\n",
       "      <td>others</td>\n",
       "      <td>way    plug      us unless  go   converter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>feedback</td>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>good case excellent value</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>feedback</td>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>great   jawbone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>complaint</td>\n",
       "      <td>negative</td>\n",
       "      <td>quality</td>\n",
       "      <td>tie  charger  conversations last   45 minutesm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>feedback</td>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>mic  great</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments     Intent Sentiment  \\\n",
       "0  So there is no way for me to plug it in here i...  complaint  negative   \n",
       "1                        Good case, Excellent value.   feedback  positive   \n",
       "2                             Great for the jawbone.   feedback  positive   \n",
       "3  Tied to charger for conversations lasting more...  complaint  negative   \n",
       "4                                  The mic is great.   feedback  positive   \n",
       "\n",
       "   Context                                     Content_Parsed  Intent_Codes  \n",
       "0   others         way    plug      us unless  go   converter             1  \n",
       "1  quality                          good case excellent value             0  \n",
       "2  quality                                    great   jawbone             0  \n",
       "3  quality  tie  charger  conversations last   45 minutesm...             1  \n",
       "4  quality                                         mic  great             0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Category mapping\n",
    "df['Intent_Codes'] = df['Intent']\n",
    "df = df.replace({'Intent_Codes':intent_codes})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
    "                                                    df['Intent_Codes'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385     1\n",
      "1180    1\n",
      "1407    0\n",
      "624     0\n",
      "1602    0\n",
      "       ..\n",
      "986     1\n",
      "133     0\n",
      "361     0\n",
      "1364    0\n",
      "451     0\n",
      "Name: Intent_Codes, Length: 1700, dtype: int64 1859    1\n",
      "1059    1\n",
      "114     0\n",
      "791     0\n",
      "1813    1\n",
      "       ..\n",
      "1121    1\n",
      "955     1\n",
      "1073    1\n",
      "1906    1\n",
      "271     0\n",
      "Name: Intent_Codes, Length: 300, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text representation\n",
    "\n",
    "We have various options:\n",
    "\n",
    "* Count Vectors as features\n",
    "* TF-IDF Vectors as features\n",
    "* Word Embeddings as features\n",
    "* Text / NLP based features\n",
    "* Topic Models as features\n",
    "\n",
    "We'll use **TF-IDF Vectors** as features.\n",
    "\n",
    "We have to define the different parameters:\n",
    "\n",
    "* `ngram_range`: We want to consider both unigrams and bigrams.\n",
    "* `max_df`: When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold\n",
    "* `min_df`: When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold.\n",
    "* `max_features`: If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 181)\n",
      "(300, 181)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we have fitted and then transformed the training set, but we have **only transformed** the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'complaint' category:\n",
      "  . Most correlated unigrams:\n",
      ". poor\n",
      ". waste\n",
      ". love\n",
      ". good\n",
      ". great\n",
      "  . Most correlated bigrams:\n",
      ". work great\n",
      ". waste money\n",
      "\n",
      "# 'feedback' category:\n",
      "  . Most correlated unigrams:\n",
      ". amaze\n",
      ". nice\n",
      ". good\n",
      ". love\n",
      ". great\n",
      "  . Most correlated bigrams:\n",
      ". waste money\n",
      ". work great\n",
      "\n",
      "# 'marketing' category:\n",
      "  . Most correlated unigrams:\n",
      ". item\n",
      ". buy\n",
      ". would\n",
      ". highly\n",
      ". recommend\n",
      "  . Most correlated bigrams:\n",
      ". go back\n",
      ". work well\n",
      "\n",
      "# 'spam' category:\n",
      "  . Most correlated unigrams:\n",
      ". new\n",
      ". awesome\n",
      ". night\n",
      ". know\n",
      ". stay\n",
      "  . Most correlated bigrams:\n",
      ". customer service\n",
      ". work great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(intent_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the unigrams correspond well to their category. However, bigrams do not. If we get the bigrams in our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go back',\n",
       " 'come back',\n",
       " 'sound quality',\n",
       " 'work well',\n",
       " 'waste money',\n",
       " 'customer service',\n",
       " 'work great']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are only seven. This means the unigrams have more correlation with the intent than the bigrams, and since we're restricting the number of features to the most representative 300, only a few bigrams are being considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the files we'll need in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "with open('Pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('Pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('Pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('Pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('Pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('Pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('Pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('Pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "with open('Pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
